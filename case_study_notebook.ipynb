{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyclistic Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Background\n",
    "\n",
    "As a junior data analyst working in the marketing analyst team at Cyclistic, a bike-sharing company active in Chicago, IL, my team has been tasked with understanding how casual riders and annual members use Cyclistic bikes differently. Casual riders consist of customers that purchase single-ride or full-day passes, whereas annual members subscribe yearly for unlimited biking access. The marketing director postulates that the company's future success depends on maximizing the number of yearly memberships by converting casual riders into annual members. My team will be designing a new marketing strategy which can best execute this idea with compelling data insights and professional visualizations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality assessment\n",
    "\n",
    "*Reliablity: ?*\n",
    "\n",
    "There are a lot of missing data values, but as you'll see in the data cleaning section below, we've found a way to stay in the 95% confidence range for the most part.\n",
    "\n",
    "*Original: ✔*\n",
    "\n",
    "Data is from a first party source.\n",
    "\n",
    "*Comprehensive: ✔*\n",
    "\n",
    "There are enough fields to paint a full picture of ridership statistics between members and casual riders.\n",
    "\n",
    "*Current: ✔*\n",
    "\n",
    "Data is from 2023, only one year prior to this study.\n",
    "\n",
    "*Vetted: ✔*\n",
    "\n",
    "This process is explore below, but there seem not to be too high a degree of extreme error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach (at a glance)\n",
    "\n",
    "The 6 steps to effective data analysis, according to Google, and how I have chosen to execute them for this case study:\n",
    "\n",
    "### 1.   Ask\n",
    "Business task: \n",
    "\n",
    "Convert casual riders into full members\n",
    "\n",
    "Questions: \n",
    "\n",
    "1) What are the advantages to becoming a member?\n",
    "2) How do casual/members differ in their use of Cyclistic?\n",
    "3) Is it truly an effective strategy to convert them?\n",
    "\n",
    "### 2.   Prepare\n",
    "   Data: [divvy-tripdata](https://divvy-tripdata.s3.amazonaws.com/index.html)\n",
    "\n",
    "   Tools:\n",
    "   \n",
    "   1) R Studio for initial glimpse/rough visuals\n",
    "\n",
    "2) SQL (Google BigQuery) for data cleaning, processing, and analysis\n",
    "\n",
    "3) Tableau for data visualizations\n",
    "\n",
    "4) Google Earth for satellite view\n",
    "    \n",
    "5) Visual Studio Code/Git for version control\n",
    "\n",
    "6) Google Slides for [stakeholder presentation](https://docs.google.com/presentation/d/e/2PACX-1vTivq52Psa5nbh7PH1oXxl7W_HKb_zbeqvDixBcz2ClY7gKNysm_ddSaMkkqUzQblHoiUQCZBE7XWgC/pub?start=false&loop=false&delayms=3000)\n",
    "\n",
    "    \n",
    "### 3.   Process\n",
    "1.   Data processing (combining & adding relevant fields)\n",
    "\n",
    "        |\n",
    "    ![data set schema](schema_scrnsht-1.png)\n",
    "\n",
    "        The original datasets each come with 13 unique schema, I will be adding and modifying certain fields to make analysis more streamlined and functional. This was also to make it easier to reference for visualizations.\n",
    "\n",
    "        |\n",
    "    ![new data frame schema](new_schema.png)\n",
    "\n",
    "        The new data frames keep only the most important information, and do not include former fields used to create them. For example, instead of a start_lat and end_lat, there is now just distance traveled in kilometers. However, I can still go back and reference the original data sets if I need the points.\n",
    "\n",
    "2.   Data analysis (cleaning & analyzing)\n",
    "\n",
    "        One of the first problems I ran into was the crazy amount of null records, around 25% from each quarter. Initially, I thought to remove them to be able to work with cleaner data, but I decided against that in favor of maintaining data integrity.\n",
    "\n",
    "3.   Visualization\n",
    "\n",
    "        I prefer to visualize as I go for a better understanding of trends and behaviors, so I also used R to create rough plots that I would then take a better look into with SQL.\n",
    "\n",
    "        \n",
    "        ![types of bikes by membership Q2](q2_ridership_bar_plot.png)\n",
    "\n",
    "### 4.   Analyze\n",
    "As I stated before, the majority of analysis for this case study was done in SQL (Google BigQuery), but I did also use R Studio for some initial basic analysis because it's free and doesn't require any cloud service storage. \n",
    "\n",
    "The main questions I sought to answer in the Analysis phase were: \n",
    "\n",
    "1) How do casual riders and members behave differently?\n",
    "        This was the main comparison made within the analysis and factored into the majority of queries. Finding patterns and differences between the two groups would be used to answer the remaining question. \n",
    "    \n",
    "2) Which factors seem most important to each group overall?\n",
    "        This was to see which parts of the business model are doing well already, and which could stand to be imporved overall.\n",
    "\n",
    "### 5.   Share\n",
    "Two shareables were created to end this study, the Python markdown you're reading now, and the slides presentation that is a bit broader and less detailed to respect the stakeholders' time. \n",
    "There is also a Tableau dashboard for anyone wishing to look further into the visuals.\n",
    "\n",
    "### 6.   Act\n",
    "Overall recommendations: \n",
    "1) Source more qualitative data\n",
    "2) Explore different pricing options\n",
    "3) Highlight membership perks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Processing (& notes)\n",
    "\n",
    "### Combining Data:\n",
    "\n",
    "As the analyst of this team, my first step is to aggregate all data into meaningful sets. I've chosen quarterly and an annual data set both so that everything is organized, uniform, and verifiable. Working with the smaller quarterly data sets also requires less computational power, which is good for me because I'm not working with bleeding edge equipment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "/* \n",
    "Cyclistic Case Study: Initial Setup and Quarterly Join Queries\n",
    "Q1: jan-mar, 639,424 rows\n",
    "Q2: apr-jun, 1,751,035 rows\n",
    "Q3: jul-aep, 2,205,714 rows\n",
    "*/\n",
    "SELECT \n",
    "  *\n",
    "FROM \n",
    "  `cyclistic_data.oct_23`\n",
    "UNION ALL\n",
    "\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  `cyclistic_data.nov_23` \n",
    "UNION ALL\n",
    "\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  `cyclistic_data.dec_23`;\n",
    "-- Q4: oct-dec, 1,123,704 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "--Annual joins for fully aggregated data set\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  `cyclistic_data.q1_data`\n",
    "UNION ALL\n",
    "\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  `cyclistic_data.q2_data`\n",
    "UNION ALL\n",
    "\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  `cyclistic_data.q3_data`\n",
    "UNION ALL\n",
    "\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  `cyclistic_data.q4_data`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I began with combining some of the more obvious fields, such as start_lat & start_lng into one start_point field. Doing the same with the end_points allowed me to turn 4 columns into 2, and then when creating the working data frame, start_point and end_point were used to calculate distance_km. Referencing certain aspects of ridership, such as trip durations and distance ridden could all be done with the original schema, but it would be difficult to follow along with, hence the more direct schema of the new data frames I've set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "--Creating start_point, end_point, and distance fields\n",
    "--Used Google-specific function ST_GEOGPOINT as BigQuery is not optimized for trig\n",
    "SELECT\n",
    "  * EXCEPT(start_lng, start_lat, end_lng, end_lat),\n",
    "  ROUND(ST_DISTANCE(start_point, end_point)/1000, 2) AS distance\n",
    "FROM\n",
    "  (SELECT\n",
    "    *,\n",
    "    ST_GEOGPOINT(start_lng, start_lat) AS start_point,\n",
    "    ST_GEOGPOINT(end_lng, end_lat) AS end_point\n",
    "  FROM\n",
    "    `cyclistic_data.apr_23`\n",
    "  WHERE\n",
    "    end_lat <> 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "--Calculating trip duratiion in seconds and then also formatting time\n",
    "SELECT \n",
    "  *,\n",
    "  FORMAT_TIMESTAMP(\"%T\", TIMESTAMP_SECONDS(seconds)) as trip_duration\n",
    "FROM\n",
    "  (\n",
    "  SELECT\n",
    "    *,\n",
    "    DATE_DIFF(ended_at, started_at, SECOND) AS seconds,\n",
    "  FROM `cyclistic_data.annual_data` \n",
    "  ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "--Establishing day of week field \n",
    "SELECT\n",
    "  *,\n",
    "  CASE \n",
    "    WHEN EXTRACT(DAYOFWEEK FROM started_at) = 1 THEN 'Sunday'\n",
    "    WHEN EXTRACT(DAYOFWEEK FROM started_at) = 2 THEN 'Monday'\n",
    "    WHEN EXTRACT(DAYOFWEEK FROM started_at) = 3 THEN 'Tuesday'\n",
    "    WHEN EXTRACT(DAYOFWEEK FROM started_at) = 4 THEN 'Wednesday'\n",
    "    WHEN EXTRACT(DAYOFWEEK FROM started_at) = 5 THEN 'Thursday'\n",
    "    WHEN EXTRACT(DAYOFWEEK FROM started_at) = 6 THEN 'Friday'\n",
    "    WHEN EXTRACT(DAYOFWEEK FROM started_at) = 7 THEN 'Saturday'\n",
    "  END day_of_week\n",
    "FROM \n",
    "  `cyclistic_data.annual_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data:\n",
    "\n",
    "Typically, I would remove null values in the processing stage, but if I were to do that with the annual_df I created, it would bring the records from 5,719,877 down to 4,331,707 which is a reduction of nearly 25%. I believe this large of a subset of data could skew the analysis, especially when most of the nulls don't affect the overall processing. For example, the fields with the highest amount of null values are the start/end station names/ids, but there are far fewer nulls in the start/end lat/lng columns, which means that the station fields don't really affect the calculation of the distance and duration fields.\n",
    "\n",
    "Therefore, I will be reconciling null values on an as-needed basis to better maintain data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Calculating nulls and zero values\n",
    "SELECT\n",
    "  *\n",
    "FROM \n",
    "  `magnetic-energy-424103-m2.cyclistic_data.q1_df`\n",
    "WHERE\n",
    "  distance_km IS null\n",
    "-- distance: 426, duration: 0, end_station: 93,016, ended_at: 0, member: 0\n",
    "-- only 0.7% of all entries have null distance values, dropping them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Looking for significant outliers in distance & duration fields\n",
    "SELECT\n",
    "  member_casual,\n",
    "  MAX(duration_min) AS max_duration\n",
    "FROM\n",
    "  `magnetic-energy-424103-m2.cyclistic_data.q1_df`\n",
    "GROUP BY\n",
    "  member_casual\n",
    "  -- shows max durations of 1,400.9 & 1,400.92\n",
    "SELECT\n",
    "  member_casual,\n",
    "  MIN(duration_min) AS min_duration\n",
    "FROM\n",
    "  (SELECT\n",
    "    member_casual, -- don't want 0 or negative values\n",
    "    IF(duration_min > 0.0, duration_min, null) AS duration_min\n",
    "  FROM\n",
    "    `magnetic-energy-424103-m2.cyclistic_data.q1_df`)\n",
    "GROUP BY\n",
    "  member_casual\n",
    "  --shows mins of 0.02 for both "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Comparing median to max and min < 0 to better understand outliers\n",
    "SELECT\n",
    "  DISTINCT median_duration,\n",
    "  member_casual\n",
    "FROM\n",
    "  (SELECT\n",
    "    ride_id, member_casual, duration_min, distance_km,\n",
    "    PERCENTILE_DISC(duration_min, 0.5) \n",
    "    OVER(PARTITION BY member_casual) AS median_duration\n",
    "  FROM\n",
    "    `magnetic-energy-424103-m2.cyclistic_data.q1_df`)\n",
    "ORDER BY\n",
    "  median_duration \n",
    "  --medians of 7.07 & 8.68 respectively'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Tables:\n",
    "\n",
    "The last step of processing I compartmentalized into two separate tables for each quarter and the year: xyz_data & xyz_df. The xyz_data contains all the data from each of its combined parts, albeit more streamlined, whereas the xyz_df is even more specified and thus has fewer fields. \n",
    "\n",
    "Each of these data sets were still available for the analysis phase for when I needed to call on more raw data, such as finding the geological boundaries of the area and which types of bikes were being used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "--Creating new data frames with most important fields\n",
    "SELECT\n",
    "  ride_id, started_at, ended_at, day_of_week, ROUND(seconds/60,2) AS duration_min,\n",
    "  distance AS distance_km, start_station_name, end_station_name, member_casual\n",
    "FROM\n",
    "  `cyclistic_data.q4_data`\n",
    "ORDER BY\n",
    "  ride_id DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "--Finding geological boundaries\n",
    "SELECT\n",
    "  MAX(start_lat) AS north_lmt_1,\n",
    "  MIN(start_lat) AS south_lmt_1,\n",
    "  MAX(start_lng) AS east_lmt_1,\n",
    "  MIN(start_lng) AS west_lmt_1,\n",
    "  MAX(end_lat) AS north_lmt_2,\n",
    "  MIN(end_lat) AS south_lmt_2,\n",
    "  MAX(end_lng) AS east_lmt_2,\n",
    "  MIN(end_lng) AS west_lmt_2\n",
    "FROM\n",
    "  `cyclistic_data.annual_data`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a very lengthy processing phase, the first thing I wanted to do was look for significant outliers that could skew the analysis. My findings were that just within the intitial unaltered combined data frame, there were 15% of each start_station field, 16% of each end_station field, and 0.4% of end_lat & end_lng all missing for a combined 25% of all records containing null values. Working around these took some doing, but by carefully factoring in which information was needed for each query, I managed to use about 98-99% of all data for analysis.\n",
    "\n",
    "The first step was figure out how many significant outliers there were and how much they would affect the analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers within durations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Full view of all ride durations to glimpse outliers\n",
    "SELECT\n",
    "  ride_id, member_casual, duration_min, distance_km\n",
    "FROM\n",
    "  `cyclistic_data.annual_df`\n",
    "ORDER BY\n",
    "  duration_min DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "--Factoring out trips with durations over one day & 12 hours\n",
    "SELECT\n",
    "  ride_id, member_casual, duration_min, distance_km\n",
    "FROM\n",
    "  `cyclistic_data.annual_df`\n",
    "WHERE\n",
    "  duration_min > 1440 -- 720 for 12h\n",
    "  AND distance_km > 0\n",
    "  --AND distance_km IS null\n",
    "  --AND distance_km = 0\n",
    "ORDER BY\n",
    "  duration_min DESC\n",
    "--1440: 6418 total, 182 > 0, 37 = 0, 6199 null\n",
    "--720: 9366 total, 2149 > 0, 738 = 0, 6479 null\n",
    "--Actually I want to know how looks like for several intervals, so I'll make a function query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "--Had to use standard for the function because I decided I wanted to output to look good\n",
    "#standardSQL\n",
    "CREATE TEMP FUNCTION NUMFORMAT(number FLOAT64) AS (\n",
    "  CONCAT(REGEXP_EXTRACT(cast(number as string), r'\\d*\\.\\d{1}'), ' %')\n",
    ");\n",
    "SELECT\n",
    "  total,\n",
    "    NUMFORMAT(ROUND((total/5719877)*100,1)) AS of_all_rides,\n",
    "  pos_dist,\n",
    "    NUMFORMAT(ROUND((pos_dist/total)*100,1)) AS pos_of_total,\n",
    "  no_dist,\n",
    "    NUMFORMAT(ROUND((pos_dist/total)*100,1)) AS zeros_of_total,\n",
    "  null_dist,\n",
    "    NUMFORMAT(ROUND((pos_dist/total)*100,1)) AS nulls_of_total\n",
    "FROM  (\n",
    "  SELECT\n",
    "    COUNTIF(duration_min > 1440) AS total,\n",
    "    COUNTIF(duration_min > 1440 AND distance_km > 0) AS pos_dist,\n",
    "    COUNTIF(duration_min > 1440 AND distance_km = 0) AS no_dist,\n",
    "    COUNTIF(duration_min > 1440 AND distance_km IS null) AS null_dist,\n",
    "  FROM  (\n",
    "    SELECT\n",
    "      ride_id, member_casual, duration_min, distance_km\n",
    "    FROM\n",
    "      `cyclistic_data.annual_df`\n",
    "  ))\n",
    "--Replace 1440 with whichever duration value you'd like to view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ended up wanting to know more about the rates of different outliers at different intervals to know exactly what cut-off point should be used for visualizing.\n",
    "\n",
    "For durations over 24 hours:\n",
    "![over_24h](1440_verification.png)\n",
    "\n",
    "For durations over 6 hours:\n",
    "![over_6h](360_verification.png)\n",
    "\n",
    "For durations over 90 minutes:\n",
    "![over_90min](90_verification.png)\n",
    "\n",
    "And for durations over 30 minutes:\n",
    "![over_30min](30_verification.png)\n",
    "\n",
    "Because analysts want to work within a confidence level range of 95-99%, I decided to work with durations that were below the 45 minute mark for this analysis as 95.8% of all rides were below this interval. This gives us the maximum amount of data to work with while still being at a comfortable confidence level, as you can see here:\n",
    "![over_45min](45_verification.png)\n",
    "\n",
    "This shows that of all 5,719,877 rides in 2023, only 240,242 (or 4.2%) had durations longer than 45 minutes. Within that limited scope, 97% of all null distance and 14% of all no distance rides are contained, thus leaving the remaining 95.8% of all data that much more consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers within distances traveled:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, I began looking into extreme outliers within the scope of distances traveled by riders. Assuming that only the start point and end point are recorded in a trip, it's entirely possible that riders who maybe ran a quick errand on the bikes and then brought them back to the same station would still have a total of 0km traveled, so I will not be discounting **all** zeroes within the data frame. Instead, I'll be grouping them with the null distances as they both represent trips with either ambiguous or missing data.\n",
    "\n",
    "Next, I'll be identifying the trips that were definitely possible by calculating the rates of travel by riders. According to several internet bicycle hobbiest forums, the maximum velocity of a biker who is not a professional would be around 32km/ph. Therefore, the possible_trips column will contain all trips with rates above 0km/ph and below 32km/ph as these would all definitely be acheivable for most people. \n",
    "\n",
    "Impossible trips will be those in which too vast of a distance was covered in too short of time, as this is likely the result of an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "--Identifying extreme outliers by distance traveled\n",
    "#standardSQL\n",
    "CREATE TEMP FUNCTION NUMFORMAT(number FLOAT64) AS (\n",
    "  CONCAT(REGEXP_EXTRACT(cast(number as string), r'\\d*\\.\\d{1}'), ' %')\n",
    ");\n",
    "SELECT\n",
    "  subscriber_type,\n",
    "  total_trips,\n",
    "    NUMFORMAT(ROUND((total_trips/5719877)*100,1)) AS of_all_rides,\n",
    "  possible_trips,\n",
    "    NUMFORMAT(ROUND((possible_trips/total_trips)*100,1)) AS pos_of_total,\n",
    "  impossible_trips,\n",
    "    NUMFORMAT(ROUND((impossible_trips/total_trips)*100,1)) AS impos_of_total,\n",
    "  ambiguous_trips,\n",
    "    NUMFORMAT(ROUND((ambiguous_trips/total_trips)*100,1)) AS ambigs_of_total,\n",
    "  null_trips,\n",
    "    NUMFORMAT(ROUND((null_trips/total_trips)*100,1)) AS nulls_of_total\n",
    "FROM  (\n",
    "  SELECT\n",
    "    member_casual AS subscriber_type,\n",
    "    COUNT(ride_id) AS total_trips,\n",
    "    COUNTIF(km/ph > 0 AND km/ph < 32) AS possible_trips,\n",
    "    COUNTIF(km/ph >= 32) AS impossible_trips,\n",
    "    COUNTIF(km/ph = 0) AS ambiguous_trips,\n",
    "    COUNTIF(km/ph IS null) AS null_trips,\n",
    "  FROM  (\n",
    "    SELECT\n",
    "      ride_id, member_casual, \n",
    "      distance_km AS km, \n",
    "      duration_min/60 AS ph\n",
    "    FROM\n",
    "      `cyclistic_data.annual_df`\n",
    "    WHERE\n",
    "      duration_min >= 1\n",
    "      AND duration_min IS NOT null\n",
    "    ORDER BY\n",
    "      ph ASC\n",
    "  )\n",
    "  GROUP BY \n",
    "    member_casual\n",
    "  ) \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also broke the results down by membership so that we could start comparing behaviors of casual riders and members:\n",
    "\n",
    "![dist_outliers](verify_dist_outliers-1.png)\n",
    "\n",
    "There are null values for some of the percentage columns because the amount is too far below 0.1% to round up. Regardless, though, instead of losing ~25% of all data by dropping all records with nulls, we're getting 98.4% useable data, of which we're only losing ~0.3% for sure, and needing further investigation on another 9%. All in all, 91% of original data isn't the best number to walk away with, but it's better than 75%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing casual riders to members:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll look at a basic breakdown of ridership percentages between members and casual riders to understand their standing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Basic table to see a numeric breakdown of ridership\n",
    "SELECT\n",
    "  total_trips, member_trips, casual_trips,\n",
    "  ROUND(member_trips/total_trips, 2)*100 AS member_percentage,\n",
    "  ROUND(casual_trips/total_trips, 2)*100 AS casual_percentage\n",
    "FROM (\n",
    "  SELECT\n",
    "    COUNT(ride_id) AS total_trips,\n",
    "    COUNTIF(member_casual = 'member') AS member_trips,\n",
    "    COUNTIF(member_casual = 'casual') AS casual_trips\n",
    "  FROM\n",
    "    `cyclistic_data.annual_df`\n",
    "  );\n",
    "-- 64% of all trips are by members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Viewing the distribution of rides by quarter\n",
    "SELECT\n",
    "  COUNT(q1) AS q1_rides,\n",
    "  COUNT(q2) AS q2_rides,\n",
    "  COUNT(q3) AS q3_rides,\n",
    "  COUNT(q4) AS q4_rides\n",
    "FROM (\n",
    "  SELECT\n",
    "    IF(month <= 3, ride_id, null) AS q1,\n",
    "    IF(month <= 6 AND month > 3, ride_id, null) AS q2,\n",
    "    IF(month <= 9 AND month > 6, ride_id, null) AS q3,\n",
    "    IF(month <= 12 AND month > 9, ride_id, null) AS q4\n",
    "  FROM (\n",
    "    SELECT \n",
    "      ride_id,\n",
    "      EXTRACT(MONTH FROM started_at) AS month\n",
    "    FROM \n",
    "      `cyclistic_data.annual_df` \n",
    "  )\n",
    ")\n",
    "-- shows 639,424 for q1, 1751035 for q2, 2,205,714 for q3, and 1,123,704 for q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![distribution](ride_distribution_quarter.png)\n",
    "\n",
    "(I ended up having to plot this in R because Tableau Public has a dual axis limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- A more detailed view of the above query with weekday breakdown\n",
    "#standardSQL\n",
    "CREATE TEMP FUNCTION NUMFORMAT(number FLOAT64) AS (\n",
    "  CONCAT(REGEXP_EXTRACT(cast(number as string), r'\\d*\\.\\d{1}'), ' %')\n",
    ");\n",
    "SELECT\n",
    "  day_of_week,\n",
    "  total_trips, member_trips,\n",
    "  casual_trips, --trip_date, \n",
    "  NUMFORMAT(ROUND(member_trips/total_trips,5)*100) AS member_percentage,\n",
    "  NUMFORMAT(ROUND(casual_trips/total_trips,5)*100) AS casual_percentage\n",
    "FROM  (\n",
    "  SELECT\n",
    "    COUNT(ride_id) AS total_trips,\n",
    "    COUNTIF(member_casual='member') AS member_trips,\n",
    "    COUNTIF(member_casual='casual') AS casual_trips,\n",
    "    --DATE(started_at) AS trip_date,\n",
    "    day_of_week\n",
    "  FROM\n",
    "    `cyclistic_data.annual_df`\n",
    "  WHERE\n",
    "    ride_id = ride_id\n",
    "  GROUP BY\n",
    "    day_of_week\n",
    "    --trip_date\n",
    "    )\n",
    "ORDER BY\n",
    "  total_trips DESC\n",
    "--We can see that during the week, it's about an 80:20 split to the members,\n",
    "--but on the weekends, that ratio leans toward a 70:30 split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rides_by_day](rides_by_day_member-1.png)\n",
    "\n",
    "We've learned quite a bit just from asking \"who rides more?\" and \"who rides when?\". The next step is to visualize this data so that we can have a better representation of how it all looks beyond just numbers. A quick bar chart should be more than sufficient for showing our stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![by_day](<Dashboard 1.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While members account for about 77% of total rides, they are actually closer to 80% during the week, and then slide back to about 30% during the weekend. This shows that there is a higher amount of casual riders during the weekend than during the week. How about what times they tend to use Cyclistic most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Looking at who rides when\n",
    "#standardSQL\n",
    "CREATE TEMP FUNCTION NUMFORMAT(number FLOAT64) AS (\n",
    "  CONCAT(REGEXP_EXTRACT(cast(number as string), r'\\d*\\.\\d{1}'), ' %')\n",
    ");\n",
    "SELECT\n",
    "  start_hour,\n",
    "  --day_of_week,\n",
    "  --trip_date, \n",
    "  total_trips, member_trips, casual_trips, \n",
    "  NUMFORMAT(ROUND(member_trips/total_trips,5)*100) AS member_percentage,\n",
    "  NUMFORMAT(ROUND(casual_trips/total_trips,5)*100) AS casual_percentage\n",
    "FROM  (\n",
    "  SELECT\n",
    "    --DATE(started_at) AS trip_date,\n",
    "    --day_of_week,\n",
    "    EXTRACT(HOUR FROM started_at) AS start_hour,\n",
    "    COUNT(ride_id) AS total_trips,\n",
    "    COUNTIF(member_casual='member') AS member_trips,\n",
    "    COUNTIF(member_casual='casual') AS casual_trips    \n",
    "  FROM\n",
    "    `cyclistic_data.annual_df`\n",
    "  WHERE\n",
    "    ride_id = ride_id\n",
    "  GROUP BY\n",
    "    --day_of_week,\n",
    "    --trip_date,\n",
    "    start_hour)\n",
    "ORDER BY\n",
    "  total_trips DESC\n",
    "-- Here we can actually see that while members outnumber casual riders all day long,\n",
    "-- There are actually more casuals between midnight and 4:00am"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![by_hour](by_hour-1.png)\n",
    "\n",
    "Here we can see that out of the total trips by hour that while members outnumber casual riders all day long, there are actually more casuals between midnight and 4:00am. This let's us know that people may see Cyclistic as an affordable, available, and more widely effective method of late night urban transport. It actually makes perfect sense in an urban center like Chicago, where rideshare apps are very expensive, public transport ends around 10pm, and walking is probably not a safe bet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll check out some averages in trip durations and distances between weekdays, rideable types, and month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Looking at ridership stats (distance + duration) of the 3 bike types \n",
    "SELECT\n",
    "  rideable_type,\n",
    "  ROUND(AVG(seconds)/60,2) AS avg_duration,\n",
    "  ROUND(AVG(distance),2) AS avg_distance\n",
    "FROM \n",
    "  `cyclistic_data.q1_data` \n",
    "WHERE\n",
    "  seconds < 2700\n",
    "  --the working limit of trip durations (<45min)\n",
    "GROUP BY\n",
    "  1\n",
    "LIMIT 9\n",
    "--data consistent with other avg ridership stats except for one thing:\n",
    "--the returned table showed a ridiculously high avg duration on docked_bike\n",
    "--which is only used by casual riders, so I queried just those"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bike_types](year_ridership_bar_plot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Looking at ridership distance stats by membership type along day of week\n",
    "SELECT\n",
    "    day_of_week,\n",
    "    ROUND(AVG(distance_km),2) AS avg_ride_distance,\n",
    "    ROUND(AVG(members),2) AS avg_member_dist,\n",
    "    ROUND(AVG(casuals),2) AS avg_casual_dist\n",
    "FROM\n",
    "    (SELECT\n",
    "      member_casual, day_of_week, distance_km, \n",
    "      IF(member_casual = 'member', distance_km, null) AS members,\n",
    "      IF(member_casual = 'casual', distance_km, null) AS casuals\n",
    "    FROM\n",
    "    `cyclistic_data.q1_df`)\n",
    "GROUP BY\n",
    "   day_of_week\n",
    "ORDER BY\n",
    "  avg_ride_distance DESC\n",
    "--Seems that Sundays are the most popular and Fridays are the least for everyone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avg_dist](avg_dist-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- And the same query with average trip durations \n",
    "SELECT\n",
    "    day_of_week,\n",
    "    ROUND(AVG(duration_min),2) AS avg_ride_duration,\n",
    "    ROUND(AVG(members),2) AS avg_member_drtn,\n",
    "    ROUND(AVG(casuals),2) AS avg_casual_drtn\n",
    "FROM\n",
    "    (SELECT\n",
    "      member_casual, day_of_week, duration_min, \n",
    "      IF(member_casual = 'member', duration_min, null) AS members,\n",
    "      IF(member_casual = 'casual', duration_min, null) AS casuals\n",
    "    FROM\n",
    "    `cyclistic_data.q1_df`)\n",
    "GROUP BY\n",
    "   day_of_week\n",
    "ORDER BY\n",
    "  day_of_week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![drtn_avg_med](cllag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our detailed analysis of provided data, we have been able to see the differences and similarities in behaviors of both casual riders and members. Overall, there are far more similarities than stark contrasts in member and casual rider tendencies, however there were some key divergences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For similarities:\n",
    "\n",
    "Both riders tended to follow the same trends as far as standard distribution of rides along a 24hr scale, 7 day scale, and quarterly/seasonal scale. They also both tend to average trips around the same distances and durations, with casual members taking marginally longer to travel a negleiibly further distance per trip. \n",
    "\n",
    "They also both seem to favor electic bikes slightly more than classic bikes, and each appear to favor the same locations in the downtown area as we can see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "--Looking at the top 10 most popular station combos between members & casuals\n",
    "SELECT\n",
    "    start_station_name, end_station_name,\n",
    "    COUNT(*) AS combo,\n",
    "    COUNTIF(member_casual='member') AS member_combo,\n",
    "    COUNTIF(member_casual='casual') AS casual_combo\n",
    "FROM \n",
    "    `cyclistic_data.annual_df`\n",
    "WHERE\n",
    "    start_station_name = start_station_name\n",
    "    AND end_station_name = end_station_name\n",
    "    AND end_station_name IS NOT null\n",
    "    AND start_station_name IS NOT null\n",
    "GROUP BY\n",
    "    1,2\n",
    "ORDER BY\n",
    "    combo DESC\n",
    "LIMIT\n",
    "    10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![top_stations](station_combos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![map](cyclistic_range.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For differences:\n",
    "\n",
    "Within the standard distributions for rides taken between members and casual riders, two of the biggest differences are that casual riders tend to prefer weekends and late night rides somewhat more than members. This could be due to any number of reasons, such as: events in downtown, response to rideshare surge rates, lack of public transit after hours, or any number of other environmental factors.\n",
    "\n",
    "Either way, doing some more research into this difference could yield more exact data with which we could analyze and extrapolate a definitive reason. With the intent of converting more casual rides to members, this would be the area in which I focus my next analysis.\n",
    "\n",
    "The only other major difference between the two groups was found in the preference of rideable type. It seems that docked bikes are either only available to casual riders, or 100% ignored by members, but either way, they only make up 2.8% of all rides taken by casual riders. It's worth looking into if for nothing else than to better understand customer tendencies, but not a huge factor in this study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *A move toward qualitative data*\n",
    "\n",
    "As it stands, all of the data analyzed thus far has been entirely quanititative, which means that there is a fair amount of guesswork to be done in deciphering customer behavior and desires. These data sets have been useful for identifying significant outliers (due to human or technical error), finding averages in ridership and ride data, and locating popular customer hubs.\n",
    "\n",
    "However, being able to qualify these data with context could tell us a lot more about what makes Cyclistic attractive to customers and how we could improve the experience. For example: including error reports with data sets could inform us of a recurring issue with our devices or their ease of use. I will give several other examples below with the top three next steps for this company.\n",
    "\n",
    "#### *1) Customer surveys*\n",
    "\n",
    "Finding out who our customers are and why they use our app will help us better understand user demographics. This will allow us to better cater towards specific groups, which in turn will help to draw in more people in that area. The surveys don't need to be extensive, just enough to give us age, occupation, and reason for interest. If we want to get an even better understanding and recieve more reviews, we could offer something like \"the next ride free!\" in exchange for a review on the app. Each of these would be quick and easy ways to make our future data more qualitative.\n",
    "\n",
    "#### *2) Alternative pricing models*\n",
    "\n",
    "With step one completed, we could start being more proactive with our marketing instead of responsive. Once we understand their interests better, we may find that membership is just not that attractive of an offer. In this case, we should be open to looking to alternative pricing options.\n",
    "\n",
    "If, for example, we find that a high percentage of casual riders use our services for local downtown events as a way to avoid parking and rideshare costs, we could offer a competetively priced weekend package. This saves the customer money, yields more revenue than just one or two rides, and could potentially become an effective word-of-mouth marketing opportunity. \n",
    "\n",
    "#### *3) Differentiating membership experience*\n",
    "\n",
    "Given just the quantitative data we've explored thus far, there doesn't seem to be much of a reason to become a member outside of a potentially lower pricepoint. For local commuters within the average distance curve, this is probably a good enough reason to save a little bit of money, but for anyone else, the offer may not seem worth it. \n",
    "\n",
    "Highlighting the perks of membership within and outside of the app would be a good way to get riders to at least look into getting a membership. Maybe members' top stations and locations are always stocked during surge times during the day, or maybe they have access to an exclusive bike model that's faster or more comfortable. Regardless of what perks members get, it's crucial that we display the difference for our riders so that they have more of a reason to explore membership. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
